---
license: apache-2.0
language: zh
tags:
  - Qwen
  - LLM
  - LoRA
  - Fine-tuned
  - Instruction Tuning
pipeline_tag: text-generation
---

# ğŸ“Œ English Overview: Qwen-7B LoRA Fine-tuned Model (Chinese Instruction Tuning)

This model is fine-tuned from Alibabaâ€™s Qwen-7B-Chat using LoRA technique on the Alpaca-Zh-51k dataset. It is suitable for instruction-following tasks in Chinese.

(I found that after making adjustments to Chat model, the effect actually got worse. Perhaps making adjustments to the base version would be better)

## ğŸ§¾ Model Information

* **Base model**: [`Qwen/Qwen-7B-Chat`](https://huggingface.co/Qwen/Qwen-7B-Chat)
* **Tuning method**: LoRA (via `peft`)
* **Dataset**: Alpaca-Zh-51k
* **Training script**: `train_qwen7b_lora.py`
* **Inference script**: `test_compare.py`
* âš ï¸ This repository includes only LoRA adapter weights, not the original base model.

## ğŸš€ Usage Example

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

model_name = "Josh1207/qwen7b-alpaca-lora"

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
base_model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", trust_remote_code=True)
model = PeftModel.from_pretrained(base_model, model_name)

prompt = "æŒ‡ä»¤: è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"
inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
outputs = model.generate(**inputs, max_new_tokens=512)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

---
# ğŸ“Œ ä¸­æ–‡ç®€ä»‹ï¼šQwen-7B LoRA å¾®è°ƒæ¨¡å‹ï¼ˆä¸­æ–‡æŒ‡ä»¤å¾®è°ƒï¼‰

æœ¬æ¨¡å‹åŸºäºé˜¿é‡Œå·´å·´é€šä¹‰åƒé—® Qwen-7B-Chatï¼Œé‡‡ç”¨ LoRA æŠ€æœ¯ï¼Œä½¿ç”¨ Alpaca-Zh-51k æ•°æ®é›†è¿›è¡Œäº†ä¸­æ–‡æŒ‡ä»¤å¾®è°ƒï¼Œé€‚ç”¨äºä¸­æ–‡ä»»åŠ¡çš„ç†è§£ä¸ç”Ÿæˆã€‚

æ³¨ï¼š å¯¹Chatè¿›è¡Œå¾®è°ƒåæ•ˆæœåè€Œå˜å·®äº†ï¼Œæˆ–è®¸å¯¹baseç‰ˆæœ¬å¾®è°ƒä¼šå¥½ä¸€äº›

## ğŸ§¾ æ¨¡å‹ä¿¡æ¯

- **åŸºåº§æ¨¡å‹**ï¼š[`Qwen/Qwen-7B-Chat`](https://huggingface.co/Qwen/Qwen-7B-Chat)
- **å¾®è°ƒæ–¹æ³•**ï¼šLoRAï¼ˆä½¿ç”¨ PEFT åº“ï¼‰
- **è®­ç»ƒæ•°æ®é›†**ï¼šAlpaca-Zh-51k
- **è®­ç»ƒè„šæœ¬**ï¼š`train_qwen7b_lora.py`
- **æ¨ç†è„šæœ¬**ï¼š`test_compare.py`
- âš ï¸ æœ¬æ¨¡å‹ä»…åŒ…å« LoRA adapterï¼Œä¸åŒ…å«åŸå§‹åŸºåº§æƒé‡

## ğŸš€ ä½¿ç”¨ç¤ºä¾‹

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

model_name = "Josh1207/qwen7b-alpaca-lora"

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
base_model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", trust_remote_code=True)
model = PeftModel.from_pretrained(base_model, model_name)

prompt = "æŒ‡ä»¤: è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"
inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
outputs = model.generate(**inputs, max_new_tokens=512)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
````


---


